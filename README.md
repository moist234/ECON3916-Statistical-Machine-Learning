# ğŸ“Š Economic Data Science Portfolio

Welcome! This repository serves as my digital portfolio for **ECON 3916: Statistical & Machine Learning for Economics**, where I explore how modern data science techniques can extend and enhance traditional economic analysis.

---

## ğŸ‘‹ About Me

I am an undergraduate economics student with strong interests in **Data Analysis, Economic Consulting, and Finance**.  
My academic focus is on bridging the gap between **economic theory** and **practical, data-driven decision making**.

Through this portfolio, I aim to demonstrate my ability to:
- Apply rigorous economic reasoning
- Work comfortably with real-world data
- Translate statistical results into meaningful economic insights

---

## ğŸ“ About This Portfolio

This repository contains my **coursework, labs, and projects** for ECON 3916.

The course follows a **Concept Extension philosophy**:
- We begin with foundational statistical tools used in economics (e.g., linear regression, hypothesis testing).
- We then **scale these ideas** using modern Machine Learning methods (e.g., Lasso, regularization, predictive modeling).

A key theme throughout my work is learning how to **combine causal inference with predictive analytics** â€” understanding not just *what* predicts outcomes, but *why* those relationships exist.

---

## ğŸ› ï¸ Tech Stack

I am actively developing proficiency with the following tools and libraries:

- ğŸ **Python**
- ğŸ“Š **Pandas**
- ğŸ¤– **Scikit-learn**
- ğŸ“ˆ **Statsmodels**
- â˜ï¸ **Google Colab**

These tools allow me to move seamlessly from data cleaning and exploration to statistical modeling and machine learning.

---

## ğŸš€ What You'll Find Here

- Applied statistical analyses
- Machine learning extensions of economic models
- Clean, reproducible notebooks
- Clear interpretations of results from an economic perspective

Thanks for visiting â€” Iâ€™m always learning, iterating, and improving.

Netflix and other large-scale experimentation platforms have moved beyond the academic convention of p < 0.05 toward what practitioners call return-aware experimentation â€” where decision thresholds are calibrated to the business cost of each error type, not a universal statistical convention. A false positive on a low-stakes UI test carries a fundamentally different cost than one on a pricing algorithm or content ranking system, so the acceptable alpha level shifts accordingly. This reflects a broader principle I internalize in my own analytical work: decision thresholds are business parameters first, statistical conventions second. The p-value tells you the probability of your data under the null â€” it is the business context that tells you whether that probability is good enough to act on.


